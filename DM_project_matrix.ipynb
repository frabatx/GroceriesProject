{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groceries = pd.read_csv('groceries/groceries - groceries.csv', delimiter=',')\n",
    "\n",
    "with open('recipe-ingredients-dataset/train.json', 'r') as f:\n",
    "    txt = f.read()\n",
    "recipes = pd.DataFrame(json.loads(txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check ingredients that appear at least 100 times\n",
    "from collections import Counter\n",
    "\n",
    "temp_l = recipes[\"ingredients\"]\n",
    "temp_l = [j for i in temp_l for j in i]\n",
    "cnt = Counter(temp_l)\n",
    "\n",
    "ing_value_count = cnt.most_common()\n",
    "ing_value_count.sort(key= lambda x: x[1], reverse=True)\n",
    "ing = [(x,y) for x,y in ing_value_count if y>=100]\n",
    "len(ing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of ingredients that appear at least 100 times\n",
    "ing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove stopwords \n",
    "import re\n",
    "\n",
    "stopwords = 'confectioners cloves ground black all-purpose kosher green large unsalted extra-virgin purple grated dried chopped \\\n",
    "fresh diced minced brown extract dry white yellow boneless skinless breasts breast flat freshly sea dried \\\n",
    "granulated crushed shredded whole halves peeled coarse warm yolks cold flakes whites cooking fat free less sodium \\\n",
    "sliced finely low light cracked spring lean cooked part-skim roasted sliced flank unsweetened sweetened melted fine \\\n",
    "yellow dark smoked toasted crumbles plain evaporated baking firm red 1% low-fat \\\n",
    "reduced-fat italian greek style whole peeled shoulder plain unbleached dry baby and frozen\\\n",
    "button cremini pure reduced steamed nonfat & de medium extra prepared bow-tie pepper salt'\n",
    "\n",
    "stopwords = stopwords.split(' ')\n",
    "\n",
    "def remove_stop_words(query):\n",
    "    querywords = query.split()\n",
    "    results_word = [word.lower() for word in querywords if word.lower() not in stopwords]\n",
    "    results = ' '.join(results_word)\n",
    "    return results\n",
    "\n",
    "\n",
    "ingredients_filtered = []\n",
    "\n",
    "for i in recipes[\"ingredients\"]:\n",
    "    temp_l = []\n",
    "    for j in i:\n",
    "        temp_l.append(remove_stop_words(j))\n",
    "        \n",
    "    ingredients_filtered.append(temp_l)\n",
    "    \n",
    "recipes[\"ingredients_filtered\"] = ingredients_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove empty strings from ingredients_filtered\n",
    "def remove_empty_strings(l):\n",
    "    l = [i for i in l if i]\n",
    "    return l\n",
    "\n",
    "recipes[\"ingredients_filtered\"] = recipes[\"ingredients_filtered\"].apply(remove_empty_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing lemma with token\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "# Ritorna il giusto tag per la parola che analizza (se verbo lo tratta da verbo, se nome da nome, avverbio da avverbio)\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Init Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "ingredients_filtered_clean = []\n",
    "\n",
    "for sentence in recipes['ingredients_filtered']:\n",
    "    ingredients_filtered_clean.append([lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in sentence])\n",
    "\n",
    "recipes[\"ingredients_filtered_clean\"] = ingredients_filtered_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replacing plural form with singluar and fixing some mispelling errors\n",
    "\n",
    "def plural_to_sigular(string):\n",
    "    \n",
    "    res = string.replace('onions', 'onion').replace('olives', 'olive').replace('almonds', 'almond').replace('ribs', 'rib').replace('thighs', 'thigh').replace('potatoes', 'potato').replace('mushrooms', 'mushroom').replace('legs', 'leg').replace('carrots', 'carrot').replace('wings', 'wing').replace('steaks', 'steak').replace('eggs', 'egg').replace('bourbon whiskey', 'whiskey').replace('yoghurt', 'yogurt').replace('jack daniels', 'whiskey')\n",
    "    print(fatto)\n",
    "    return res\n",
    "\n",
    "ingredients_filtered_clean = []\n",
    "\n",
    "for i in recipes[\"ingredients_filtered\"]:\n",
    "        temp_l = []\n",
    "        for j in i:\n",
    "            temp_l.append(plural_to_sigular(j))\n",
    "        \n",
    "        ingredients_filtered_clean.append(temp_l)\n",
    "\n",
    "recipes[\"ingredients_filtered_clean\"] = ingredients_filtered_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check ingredients count after cleaning procedure\n",
    "temp_l = recipes[\"ingredients_filtered_clean\"]\n",
    "temp_l = [j for i in temp_l for j in i]\n",
    "cnt = Counter(temp_l)\n",
    "\n",
    "ing_value_count = cnt.most_common()\n",
    "ing_value_count.sort(key= lambda x: x[1], reverse=True)\n",
    "top_ing_clean = [(x,y) for x,y in ing_value_count if y>=100]\n",
    "len(top_ing_clean)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "top_ing_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a list of the top ingredients (keeping all ingredients)\n",
    "top_ing_clean_l = [x[0] for x in top_ing_clean]\n",
    "\n",
    "#(keeping only the top 400)\n",
    "#top_ing_clean_l = top_ing_clean_l[0:400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter out the ingredients keeping only the top ingredients\n",
    "\n",
    "def filter_ingredients(top_ing, ing):\n",
    "    ing_filtered = [i for i in ing if i in top_ing]\n",
    "    return ing_filtered\n",
    "\n",
    "ingredients_more_filtered = []\n",
    "for i in recipes[\"ingredients_filtered_clean\"]:\n",
    "    ingredients_more_filtered.append(filter_ingredients(top_ing_clean_l, i))\n",
    "    \n",
    "recipes[\"ingredients_more_filtered\"] = ingredients_more_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#after filtering the ingredients, some recipes are now empty, we remove those empty lists:\n",
    "boolean_filter = recipes[\"ingredients_more_filtered\"].str.len() == 0\n",
    "recipes = recipes[boolean_filter == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Recipe-Ingredients Matrix\n",
    "recipes_filtered_df = []\n",
    "\n",
    "for i in recipes[\"ingredients_more_filtered\"]:\n",
    "    bin_array = []\n",
    "    for j in top_ing_clean_l:\n",
    "        if j in i:\n",
    "            bin_array.append(1)\n",
    "        else:\n",
    "            bin_array.append(0)\n",
    "    recipes_filtered_df.append(bin_array)\n",
    "    \n",
    "recipes_filtered_df = pd.DataFrame(recipes_filtered_df, columns=top_ing_clean_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes_filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply PCA to this dataset\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "pca.fit(recipes_filtered_df)\n",
    "\n",
    "pca_sample = pca.transform(recipes_filtered_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cluster based on K=10\n",
    "from sklearn.cluster import KMeans\n",
    "k = 10\n",
    "kmeans10 = KMeans(n_clusters=k).fit(recipes_filtered_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Top terms per cluster:\")\n",
    "order_centroidsK10 = kmeans10.cluster_centers_.argsort()[:, ::-1]\n",
    "termsK10 = top_ing_clean_l\n",
    "for i in range(k):\n",
    "    top_wordsK10 = [termsK10[ind] for ind in order_centroidsK10[i, :30]]\n",
    "    print(\"Cluster {}: {}\".format(i, ', '.join(top_wordsK10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mglearn\n",
    "\n",
    "#Plot k-means 10 clusters on PCA\n",
    "plt.figure(figsize=(13,10))\n",
    "mglearn.discrete_scatter(pca_sample[:,0], pca_sample[:,1], kmeans10.labels_, alpha = 0.5)\n",
    "plt.legend(['cluster0', 'cluster1', 'cluster2', 'cluster3', 'cluster4',\n",
    "       'cluster5', 'cluster6', 'cluster7', 'cluster8', 'cluster9'], bbox_to_anchor=(1.02, 1), loc=2)\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.title(\"K-means (10) on PCA\")\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cluster based on K=50\n",
    "from sklearn.cluster import KMeans\n",
    "k = 50\n",
    "kmeans50 = KMeans(n_clusters=k).fit(recipes_filtered_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Top terms per cluster:\")\n",
    "order_centroidsK50 = kmeans50.cluster_centers_.argsort()[:, ::-1]\n",
    "termsK50 = top_ing_clean_l\n",
    "for i in range(k):\n",
    "    top_wordsK50 = [termsK50[ind] for ind in order_centroidsK50[i, :30]]\n",
    "    print(\"/n Cluster {}: {}\".format(i, ', '.join(top_wordsK50)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot k-means 50 clusters on PCA\n",
    "plt.figure(figsize=(13,10))\n",
    "mglearn.discrete_scatter(pca_sample[:,0], pca_sample[:,1], kmeans50.labels_, alpha = 0.5)\n",
    "plt.legend(['cluster0', 'cluster1', 'cluster2', 'cluster3', 'cluster4',\n",
    "       'cluster5', 'cluster6', 'cluster7', 'cluster8', 'cluster9', 'cluster10',\n",
    "       'cluster11', 'cluster12', 'cluster13', 'cluster14', 'cluster15',\n",
    "       'cluster16', 'cluster17', 'cluster18', 'cluster19', 'cluster20', 'cluster21', 'cluster22', 'cluster23', 'cluster24',\n",
    "       'cluster25', 'cluster26', 'cluster27', 'cluster28', 'cluster29', 'cluster30',\n",
    "       'cluster31', 'cluster32', 'cluster33', 'cluster34', 'cluster35',\n",
    "       'cluster36', 'cluster37', 'cluster38', 'cluster39', 'cluster40', 'cluster41', 'cluster42', 'cluster43', 'cluster44',\n",
    "       'cluster45', 'cluster46', 'cluster47', 'cluster48', 'cluster49'], bbox_to_anchor=(1.02, 1), loc=2)\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.title(\"K-means (20) cluster on PCA\")\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot cuisines on PCA\n",
    "plt.figure(figsize=(13,10))\n",
    "mglearn.discrete_scatter(pca_sample[:,0], pca_sample[:,1], recipes.cuisine, alpha = 0.5)\n",
    "plt.legend(['greek', 'southern_us', 'filipino', 'indian', 'jamaican',\n",
    "       'spanish', 'italian', 'mexican', 'chinese', 'british', 'thai',\n",
    "       'vietnamese', 'cajun_creole', 'brazilian', 'french', 'japanese',\n",
    "       'irish', 'korean', 'moroccan', 'russian'], bbox_to_anchor=(1.02, 1), loc=2)\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.title(\"Cuisines on PCA\")\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation for market basekt dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Market Basket dataset to arrays\n",
    "groceries_array = groceries.values\n",
    "\n",
    "# Remove NaN from baskets_arrays\n",
    "groceries_array_clean = []\n",
    "for i in groceries_array:\n",
    "   groceries_array_clean.append([x for x in i if str(x) !='nan'])\n",
    "\n",
    "df_groceries_clean = pd.DataFrame()\n",
    "df_groceries_clean[\"items\"] = groceries_array_clean\n",
    "\n",
    "#remove first element from list (the number of elements)\n",
    "for index, row in df_groceries_clean.iterrows():\n",
    "    row['items'].pop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_groceries_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check unique elements in Market basekt dataset\n",
    "baskets = list(df_groceries_clean[\"items\"])\n",
    "unique_ele = set(x for l in baskets for x in l)\n",
    "len(unique_ele)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#impostare lo stesso formato di dataset che abbiamo per le ricette\n",
    "#Remove stopwords \n",
    "import re\n",
    "\n",
    "stopwords = 'confectioners cloves ground black all-purpose kosher green large unsalted extra-virgin purple grated dried chopped \\\n",
    "fresh diced minced brown extract dry white yellow boneless skinless breasts breast flat freshly sea dried \\\n",
    "granulated crushed shredded whole halves peeled coarse warm yolks cold flakes whites cooking fat free less sodium \\\n",
    "sliced finely low light cracked spring lean citrus cooked part-skim roasted sliced flank unsweetened sweetened melted fine \\\n",
    "yellow dark smoked boiling toasted crumbles plain evaporated baking firm red 1% low-fat cake \\\n",
    "button cremini pure reduced steamed nonfat & de medium specialty white whole spread root other semi-finished bottled \\\n",
    "canned frozen \"(appetizer)\" packaged instant misc. '\n",
    "\n",
    "stopwords = stopwords.split(' ')\n",
    "\n",
    "def remove_stop_words(query):\n",
    "    querywords = query.split()\n",
    "    results_word = [word.lower() for word in querywords if word.lower() not in stopwords]\n",
    "    results = ' '.join(results_word)\n",
    "    return results\n",
    "\n",
    "\n",
    "ingredients_filtered = []\n",
    "\n",
    "for i in df_groceries_clean[\"items\"]:\n",
    "    temp_l = []\n",
    "    for j in i:\n",
    "        temp_l.append(remove_stop_words(j))\n",
    "        \n",
    "    ingredients_filtered.append(temp_l)\n",
    "    \n",
    "df_groceries_clean[\"items_filtered\"] = ingredients_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove empty strings from ingredients_filtered\n",
    "def remove_empty_strings(l):\n",
    "    l = [i for i in l if i]\n",
    "    return l\n",
    "\n",
    "df_groceries_clean[\"items_filtered\"] = df_groceries_clean[\"items_filtered\"].apply(remove_empty_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_groceries_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check unique elements in Market basekt dataset\n",
    "baskets = list(df_groceries_clean[\"items_filtered\"])\n",
    "unique_mb = set(x for l in baskets for x in l)\n",
    "len(unique_mb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing lemma with token\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "# Ritorna il giusto tag per la parola che analizza (se verbo lo tratta da verbo, se nome da nome, avverbio da avverbio)\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing lemma with token\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "items_filtered_clean = []\n",
    "\n",
    "for sentence in df_groceries_clean['items_filtered']:\n",
    "    items_filtered_clean.append([lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in sentence])\n",
    "\n",
    "df_groceries_clean[\"items_filtered_clean\"] = items_filtered_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_groceries_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check unique elements in recipes dataset\n",
    "baskets = list(recipes[\"ingredients_more_filtered\"])\n",
    "unique_recipes = set(x for l in baskets for x in l)\n",
    "len(unique_recipes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# controllo gli elementi in comune tra i due dataset puliti\n",
    "common_items =[]\n",
    "for items in unique_recipes:\n",
    "    if items in unique_mb:\n",
    "        common_items.append(items)\n",
    "\n",
    "len(common_items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converto vegetali e frutta in recipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import WikipediaExtraction as we\n",
    "\n",
    "fruits = we.getFruits()\n",
    "vegetables = we.getVegetables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for recipe in recipes[\"ingredients_more_filtered\"]:\n",
    "    for index, ingredient in enumerate(recipe):\n",
    "        if(ingredient in fruits):\n",
    "            recipe[index] = \"fruits\"\n",
    "        if(\"fruit\" in ingredient or \"fruits\" in ingredient):\n",
    "            recipe[index] = \"fruits\"\n",
    "        if(ingredient in vegetables):\n",
    "            recipe[index] = \"vegetables\"\n",
    "        if(\"vegetable\" in ingredient or \"vegetables\" in ingredient):\n",
    "            recipe[index] = \"vegetables\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check unique elements in recipes dataset\n",
    "baskets = list(recipes[\"ingredients_more_filtered\"])\n",
    "unique_recipes = set(x for l in baskets for x in l)\n",
    "len(unique_recipes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notiamo che abbiamo eliminato 40 elementi unici all'interno del dataset recipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check unique elements in recipes dataset\n",
    "baskets = list(recipes[\"ingredients_more_filtered\"])\n",
    "unique_recipes = set(x for l in baskets for x in l)\n",
    "len(unique_recipes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converto vegetali e frutta su groceries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for recipe in df_groceries_clean[\"items_filtered_clean\"]:\n",
    "    for index, ingredient in enumerate(recipe):\n",
    "        #print(\"Ingrediente prima: {0}\".format(ingredient))\n",
    "        if(ingredient in fruits):\n",
    "            recipe[index] = \"fruits\"\n",
    "        if(\"fruit\" in ingredient or \"fruits\" in ingredient):\n",
    "            recipe[index] = \"fruits\"\n",
    "            #print(\"Frutta: {0}\".format(ingredient))\n",
    "        if(ingredient in vegetables):\n",
    "            recipe[index] = \"vegetables\"\n",
    "        if(\"vegetable\" in ingredient or \"vegetables\" in ingredient):\n",
    "            recipe[index] = \"vegetables\"\n",
    "            #print(\"Vegetale: {0}\".format(ingredient))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check unique elements in Market basekt dataset\n",
    "baskets = list(df_groceries_clean[\"items_filtered_clean\"])\n",
    "unique_mb = set(x for l in baskets for x in l)\n",
    "len(unique_mb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# controllo gli elementi in comune tra i due dataset puliti\n",
    "common_items =[]\n",
    "for items in unique_recipes:\n",
    "    if items in unique_mb:\n",
    "        common_items.append(items)\n",
    "\n",
    "len(common_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
